\documentclass{report}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{pdfpages}
\title{HARE: Final Report}
\author{Sandia National Labs, IBM, Bell Labs, Vita Nuova
\thanks{\protect\includegraphics[height=0.3in]{thunderchicken}%
}%
\thanks{Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Department of Energy’s National Nuclear Security Administration under contract DE­AC04­94AL85000. SAND- 2011-xxxxC.}  
}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\pagebreak

This report documents the results of work done over a 6 year period under the FAST-OS 
programs. The first effort was called Right-Weight Kernels, (RWK) and was concerned with 
improving measurements of OS noise so it could be treated quantitatively; 
and evaluating the use of two operating systems, Linux
and Plan~9, on HPC systems and determining how these operating systems needed
to be extended or changed for HPC, while still retaining their general-purpose nature. 

The second program, HARE, explored the creation of alternative runtime models, 
building on RWK. All of the HARE work was done on Plan~9. The HARE reseachers
were mindful of the very good Linux and LWK work being done at other labs and saw no
need to recreate it. 

The organizations included LANL (RWK) and Sandia (RWK, HARE), as the PI moved to Sandia; 
IBM; Bell Labs; and Vita Nuova, as a subcontractor to Bell Labs. In any given year, 
the funding was suffcient to cover a PI from each organization part time. 

Even given this limited funding, the two efforts had outsized impact: 
\begin{itemize}
\item Helped Cray decide to use Linux, instead of a custom kernel, and provided
the tools needed to make Linux perform well
\item Created a successor operating system to Plan~9, NIX, which has been taken in 
by Bell Labs for further development
\item Created a standard system measurement tool, Fixed Time Quantum or FTQ, which is widely
used for measuring operating systems impact on applications
\item Spurred the use of the 9p protocol in several organizations, including IBM
\item Built software in use at many companies, including IBM, Cray, and Google
\item Spurred the creation of alternative runtimes for use on HPC systems
\item Demonstrated that, with proper modifications, a general purpose operating systems
can provide communications up to 3 times as effective as user-level libraries
\end{itemize}

Open source was a key part of this work. The code developed for this project 
is in wide use and available at many places. The core Blue Gene code is 
available at https://bitbucket.org/ericvh/hare. 

We describe details of these impacts in the following sections. The rest of this 
report is organized as follows: first, we describe commercial impact; next, 
we describe the FTQ benchmark and its impact in more detail; operating systems
and runtime research follows; we discuss infrastructure software; and close with a 
description of the new NIX  operating system, future work, and conclusions. 
\input{commercial}
\input{ftq}
\input{os}
\input{runtime}
\input{infrastructure}

\chapter{NIX}

NIX is an operating system designed for manycore CPUs in which not 
all cores are capable of running an operating system. Examples of such 
systems abound, most recently in the various GPU systems. While most 
heterogeneous systems treat the non-OS cores as a subordinate system, 
in NIX they are treated as peers (or even superiors) of the 
OS cores. 
Many  additional features of NIX were created based on what we learned from 
the RWK and HARE projects. 

NIX
features a heterogeneous CPU model and can use a shared address space
if it is available. 
NIX partitions cores by function: Timesharing Cores, or
TCs; Application Cores, or ACs; and Kernel Cores, or KCs.  One or more
TC runs traditional applications.  KCs are optional, running kernel
functions on demand.  ACs are also optional, devoted to running an
application with no interrupts; not even clock interrupts.  Unlike
traditional kernels, functions are not static: the number of TCs, KCs,
and ACs can change as needed.  Also unlike traditional
systems, applications can communicate by sending messages to the TC
kernel, instead of system call traps.  These messages are "active"
taking advantage of the shared-memory nature of manycore CPUs to pass
pointers to data and code to coordinate cores.

NIX has transparent support for a near-unlimited range of page sizes. 
On the 64-bit x86 family, for example, NIX supports 2 MiB and 1 GiB pages. 
Support for 1 GiB pages is crucial to performance: on one popular 
graph application we measured a 2x speedup when 1 GiB pages were used 
instead of 2 MiB pages. 
The implementation allows for virtual pages, e.g., one could 
provide 16KiB pages that are a concatenation of physical 4 KiB pages. 
These virtual pages would allow us to reduce the number of page 
faults an application causes, although they would not reduce 
TLB pressure. 

NIX gives us the best of many worlds. Applications can own a core, 
and will not even be preempted by a clock tick, as they are even 
on Light Weight Kernels today. At the same time, the full capability 
of a general purpose operating system is available on core nearby. 
We feel that NIX represents the shape of future operating systems
on manycore systems that are being built now. 

Further information on NIX can be found in Appendis A. The source 
code is available at https://code.google.com/p/nix-os. 

\chapter{Future Work}
%Over the period of these two projects, we have shown that with appropriate 
%modifications, general purpose operating systems can be used for HPC computers. 
%We have developed tools that show the quantitative benefit of these 
%This experience confirms the experience of industry. 
This research has suggested a number of new directions for HPC. 
\begin{itemize}
\item New operating systems that step outside the stale Light-Weight Kernel/General Kernel
argument. We have shown that one such kernel, NIX, can support the desirable attributes of both
and even outperform LWKs at the things they do best. 
\item Network IO can and should be done in the kernel, and future research should 
work out the API. OS Bypass should be bypassed. 
\item Network software that does not assume a perfect network needs to be created and put into use.
This change in turn will allow innovation to occur in 
HPC hardware. 
\item HPC hardware needs to change so that it can provide near-perfect
information about failures, but not nodes that never fail. Software 
can handle faults in a far more flexible manner than hardware. 
\item File systems must change to make explicit use of hierarchy. The ratio of Compute Nodes to IO Nodes should not be 
statically defined by wires, as it is today, but defined by the needs of the application, and hence flexible. 
\end{itemize}

\chapter{Conclusions}

While no commercial HPC vendor is using Plan~9, they are using much of the other software we developed, 
including the 64-bit compiler toolchain, FTQ, and NIX. 
There are other lessons learned: 
\begin{itemize}
\item OS bypass is not required for good network performance. ``Conventional knowledge
bypass'' is much more important: if we can get runtime libraries to accomodate 
such concepts as a shared heap address space, we can reduce runtime and OS 
complexity and remove OS bypass for the most part. 
\item The idea of quantitative measurement of OS noise was controversial when we first proposed it in 2004. 
In fact, there is a wealth of signal processing software and knowledge that can be applied to HPC. We need
to move beyond qualitative descriptions to quantitative measurements. Adjectives should be avoided
when numbers can be supplied. 
\item Users want a Unix-like API, even on systems like Blue Gene which do not run Unix on all nodes. 
Key requirements such as sockets, dynamic libraries, and a reasonably standard file system
interface can not be waved away. Light Weight Kernels that fail to provide these services
will either change (as did the CNK) or be abandoned by the vendor (as was Catamount). 
\item As pointed out above, ``Thus, for reliable communication, the hardware level flow-control is neither necessary nor sufficient, 
and flow-control must be added somewhere in the multiplexing software stack.'' Communications networks
on future architectures could be made simpler, faster, more reliable, and lower power by taking 
this fact into account. 
\item We need to move beyond simply gluing hardware interfaces into existing kernel designs in non-optimal ways. 
One of the worst examples of this tendency can be seen in the Linux drivers for HPC networks: they all emulate an Ethernet. 
This emulation results in ineffeciencies and poor software structure: 
the Jaguar system, for example, has 30,000 hardwired ARP-table entries 
{\em on each node} so that the TCP/IP address in $[x,y,z]$ form 
can be mapped to an Ethernet address in $[x,y,z]$ form. 
Ethernet interfaces are for Ethernet networks. There is no need to have a network interface for an
HPC network 
emulate an Ethernet, as it does on Compute Node Linux on the Cray or 
ZeptoOS on BG/P. 
\end{itemize}

This work was done as part of the FAST-OS program and its successor. 
The ground was prepared for the FAST-OS programs in a series of meetings held in
the eary 2000s. One of the PIs earliest talks dates from 2002. In 2003, we made
the following arguments: 
\begin{itemize}
\item Linux is fun and working now; enjoy it
\item Linux is not forever
\item Some fundamental properties of the Unix model have fatal flaws
\item We are entering a period of “architecture flux” 
\item We need diversity in OS research now to prepare for new, strange machines
\item DOE can succeed in creating a successful, diverse OS research community
\end{itemize}

We might ask: is any of this less true now that our community is faced with 
the challenge of scaling parallelism up 1000-fold? We would argue that 
the problem is even worse. While this program may have had successes, in some 
sense the community is 
still plowing the same ruts. The extant machines still largely run Linux, and the 
problems we predicted in 2003 (for the 2012 timeframe) are starting to 
come to pass: with each new version, Linux gets just a bit slower, as it grows in size 
and complexity and the processors do not get any faster. As we stated in 2003, 
``At some point, like all other OSes, it is [Linux] going to fall over''. 
Several large companies, in private conversation, have told the PI that one of the
biggest management headaches they have is dealing with the increasing complexity of 
the Linux kernel, version to version, as it gains more features and bulk.  

Did FAST-OS succeed? That is a harder question. In the sense of industry impact, we 
clearly had great success. In the sense of building a vibrant community of OS researchers
in DOE, we had some success; there is very good OS work going on at Argonne, Oak Ridge, 
and Sandia, and on a smaller scale, at other Labs. 

Those are hard-won gains, and maintaining them 
will not be easy: OS research continues to come under 
pressure for both budget reasons and 
from those who believe that industry will just supply an OS as a turnkey 
answer, in spite of all historical evidence to the contrary. No standard OS has ever worked
for the large scale without substantial measurement and change. 

Further, the challenges 
in the worlds of Google, Amazon, and Facebook are now much bigger 
and freer of past constraints: just this year Google announced 
the Exacycle initiative, and is delivering one billion core
hours for researchers. 
Further, the scale of the commercial world's file storage problems now dwarfs 
anything that is facing DOE. Hiring and keeping people at 
DOE Labs is becoming ever more challenging, absent support for 
new and novel approaches to DOE problems. People are not excited
by the idea of spending the next 10 years of their career supporting 
the legacy software produced over the last quarter century. 

DOE needs to build on the success of these programs by continuing
to fund new, innovative research that will 
help vendors set new directions. 
At the same time, DOE Labs should almost never be in the business of 
providing production software; 
nor should commercial uptake of every last bit of research software
be the standard 
by which success is measured. 
Failure should always be an option. If there are no failures, 
then the research is not nearly aggressive enough. 

When FAST-OS started, the discussion was whether the DOE Labs would have any OS 
knowledge or capability left by the end of the decade. FAST-OS and its successor
program created a culture of competency in the DOE Labs and attracted some very 
capable commercial and research partners  from outside the labs. 
In the end, we count this research, 
and the larger program that funded it, as a success. Whether
the gains we have made will be maintained is a question that can only 
be answered at the end of {\em this} decade. 

%\bibliographystyle{abbrvnat} 
\bibliographystyle{plain}

\bibliography{all}

\appendix
\appendixpage
\section*{NIX}
This document will appear in Bell Labs Technical Journal and was the subject
of a talk at a Bell Labs Technical Conference in Antwerp, Belgium, Oct. 2011. 
\newpage
\includepdf[pages=-]{bltj.pdf}

\addappheadtotoc
\end{document}
