\chapter{Operating Systems Developments}
\section{Plan 9 Port to Blue Gene /L and /P}
\subsection{Large Pages}
Blue Gene   processors use software-loaded TLBs. A TLB manages the virtual to physical mapping for a single page, which 
in both Linux and Plan~9 is 4096 bytes. The processors support more than just one page size, in multiples of 4, from 1024 bytes to 1 Gbyte (with a few sizes missing). 
The CNK 
features 1 Mbyte TLBs, and benefits from reduced TLB fault interrupts. 

We extended Plan~9 to support larger TLBs. In order to measure the improvement we used the strid3 benchmark
\begin{figure}[h]
\begin{center}
 \includegraphics[width=4in]{strid3.pdf}
 \caption{{\bf Result of running strid3 on CNK, Plan 9, and Plan 9 with 1 MByte pages. }}
\label{strid3}
\end{center}
\end{figure}
to show the impact of TLB size upon an application. Shown in Figure \ref{strid3} is the result. Plan~9 is 
eight times slower than the CNK when 4096 byte pages are used. With one Mbyte pages in Plan~9, its 
performance is identical to the CNK. 
The change to Plan~9 to support the larger pages amounted to less than 30 lines of code\cite{DBLP:journals/ife/MinnichM09}. 

\section{Currying and Process-private system calls}
Extensive measurement of the kernel, using the tool developed and described in \cite{plan9trace}, showed
that for read and write system calls a great deal of overhead was repeated for each iteration of the 
call. Two of the biggest overheads are validation of the file descriptor and the read/write pointer and 
length validation. We also found in most cases that the pointers are being reused for each call: it is extremely rare for these parameters to be wrong, yet programs pay the full overhead for checking them 
each time. For the Blue Gene global barrier device, the overhead for the system call was 3.529 microseconds. 

In \cite{currying} we describe the possible approaches to resolving this problem. We developed two 
several extensions to Plan~9 that allowed the kernel to Curry the arguments to the read and write
system calls. The result: the file descriptor and point/length information is checked once, and the
result stored in a cache held in the proc struct (per-process information) in the kernel. The per-process structure is 
also extended with a private system call table. When the process makes a private system
call, all the standard checking is skipped and the cached values are used instead. This 
new system call path resulted in a 5-fold reduction in overhead, to 729 nanoseconds. 

The Currying and per-process system calls together form a new type of interface to operating system
kernels. 

\subsection{Shared Heap}
\section{File Systems}
\subsection{Compute Node Caches}
\section{New network communications models}
\subsection{Active Message Support}
\subsection{What are we calling Charles stuff?}
\section{Kittyhawk Linux Kernel}
\subsection{Hybrid Networks}
\subsection{Communication Domains}
\subsection{Heterogenous Compute Node Kernels}
\section{Kittyhawk L4 Kernels}
\subsection{Virtual Machine Monitor for BG/P}
