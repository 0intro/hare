\chapter{Operating Systems Developments}
\section{Plan 9 Port to Blue Gene /L and /P}
\subsection{Large Pages}
Blue Gene   processors use software-loaded TLBs. A TLB manages the virtual to physical mapping for a single page, which 
in both Linux and Plan~9 is 4096 bytes. The processors support more than just one page size, in multiples of 4, from 1024 bytes to 1 Gbyte (with a few sizes missing). 
The CNK 
features 1 Mbyte TLBs, and benefits from reduced TLB fault interrupts. 

We extended Plan~9 to support larger TLBs. In order to measure the improvement we used the strid3 benchmark
\begin{figure}[h]
\begin{center}
 \includegraphics[width=4in]{strid3.pdf}
 \caption{{\bf Result of running strid3 on CNK, Plan 9, and Plan 9 with 1 MByte pages. }}
\label{strid3}
\end{center}
\end{figure}
to show the impact of TLB size upon an application. Shown in Figure \ref{strid3} is the result. Plan~9 is 
eight times slower than the CNK when 4096 byte pages are used. With one Mbyte pages in Plan~9, its 
performance is identical to the CNK. 
The change to Plan~9 to support the larger pages amounted to less than 30 lines of code\cite{DBLP:journals/ife/MinnichM09}. 

\section{Currying and Process-private system calls}
Extensive measurement of the kernel, using the tool developed and described in \cite{plan9trace}, showed
that for read and write system calls a great deal of overhead was repeated for each iteration of the 
call. Two of the biggest overheads are validation of the file descriptor and the read/write pointer and 
length validation. We also found in most cases that the pointers are being reused for each call: it is extremely rare for these parameters to be wrong, yet programs pay the full overhead for checking them 
each time. For the Blue Gene global barrier device, the overhead for the system call was 3.529 microseconds. 

In \cite{currying} we describe the possible approaches to resolving this problem. We developed two 
several extensions to Plan~9 that allowed the kernel to Curry the arguments to the read and write
system calls. The result: the file descriptor and pointer/length information is checked once, and the
result stored in a cache held in the proc struct (per-process information) in the kernel. The per-process structure is 
also extended with a private system call table. When the process makes a private system
call, all the standard checking is skipped and the pre-validated cached values are used instead. This 
new system call path resulted in a 5-fold reduction in overhead, to 729 nanoseconds. 

The Currying and per-process system calls together form a new type of interface to operating system
kernels. 

\subsection{Shared Heap}
Currying reduces the overhead for process-driven operations. It does nothing for interrupt-driven
operations, because interrupts, by design, do not occur in a process context. Page pinning is used 
to resolve this problem on Linux systems: process memory mappings are communicated to the driver
and ``pinned'' into place. Page pinning adds a great deal of code to a kernel; in some cases the
pinning code for (e.g.) Linux is larger than all of Plan~9 itself. 

We implemented a ``shared heap'' model in Plan~9. All process heaps, the kernel, and the drivers 
share a common address space, consisting of the top 1.5 Gbytes of memory. All heap (not text, stack,
or data, 
just heap) is allocated from this shared 1.5 Gbytes of memory. 
A given heap address in a process, kernel, or interrupt handler will
point to the same piece of memory. In contrast, in a standard OS, the same heap address in each 
process points to different memory. Note that a shared heap address space does not imply that 
all the data in the heap itself is shared between all the processes.  
Processes can share parts of the heap, but are typically limited to their own private piece, and 
can not see other processes heap data. 

Processes can easily pass pointers to each other because 
a given heap pointer has the same meaning in every process. Processes can communicate an address 
to the driver for use by the interrupt handler and there is no need for complex pinning, because the 
address has the same meaning in every machine context. 

We further modified the BG/P Torus driver to implement queues on the receive side. As packets are 
received  by the interrupt handler they are directly moved to the queues. 
\footnote{These queues are 
very similar to those provided on the T3D. In the T3D, however, they were implemented in hardware. }
Finally, once the driver and kernel were extended, we created a library to allow processes to 
send from the shared heap and receive into queues in the shared heap. 

Our resulting performance was better than that of the MPI library, even while using 
the kernel for I/O. In fact a simple ping-pong code ran with about 1/3 the latency 
of MPI. Our code was about 100 times smaller than the MPI library. Further, 
the device could be made to work for multiple independent applications, which is not possible
on BG/P hardware if the direct-access MPI libraries are used. We thus showed that if a 
kernel provides the proper abstractions, it is not necessary to run the network device driver
in user memory, as is done today. 

\section{File Systems}
\subsection{Compute Node Caches}

We needed to reduce the load on the file server shared by thousands of
nodes in a Blue Gene Plan 9 cluster. The intent was to execute the caching
mechanisms on the I/O nodes as well as look at spreading them out amongst 
the compute nodes depending on the I/O requirements of a particular 
application.  

Plan 9 had two existing caching mechanisms for 9P file servers: a volatile 
cache controlled by devmnt, and a persistent disk-based cache managed by 
the user-level server cfs.
Both sent the server all 9P requests except reads satisfied by the cache.
Cfs was quickly converted to a ramcfs that used the large memory of a
Blue Gene I/O node instead of a disk, but most 9P traffic still passed
through. A redesign produced fscfs, which breaks the direct link
between its client's 9P transactions and those it makes to the server,
producing a dramatic reduction in traffic seen by the server.

Fscfs is a normal, user-level C application, using libthread for internal 
concurrency. Like cfs, it is a 9P transducer: it acts as a client to a 
remote 9P file server on one connection, and acts as a 9P server to its 
client on another connection. (Once that connection is mounted in the name 
space, many client processes can share it.) To provide data caching, fscfs 
uses the same strategy as the existing caches: it makes a copy of the Â­data
as it passes through in either direction, and stores it in a local cache. 
Subsequent read requests for the same data will be satisfied from the cache. 
Write requests are passed through to the server, replacing data in the 
cache if successful. Cached data is associated with each active file, but the 
memory it occupies is managed on a least-recently-used basis across the 
whole set of files. When a specified threshold for the cache has been reached, 
the oldest cached data is discarded.

Fscfs maintains a path tree representing all the paths walked in that tree, 
successfully or 
unsuccessfully. A successful walk results in an end-point that records a
handle referring to that point in the server's hierarchy. (Note that 
intermediate names need not have server handles.) If a walk from a
given point failed at some stage, that is noted by a special path value at 
that point in the tree, which gives the error string explaining why the walk 
failed. If a subsequent walk from the client retraces an existing path, 
fscfs can send the appropriate response itself, including failures and walks 
that were only partially successful. If names remain in the walk request 
after traversing the existing path, fscfs allocates a new handle for the new 
path end-point, sends the whole request to the server, and updates the path 
appropriately from the server's response. Remembering failures is a
great help when, for instance, many processes on many nodes are enumerating 
possible names for different possible versions of (say) Python library 
files or shared libraries, most of which do not exist. (It would be more 
rational to change the soft ware not to do pointless searches, but it is not 
always possible to change components from standard distributions.)

Fscfs is just over 2,000 lines of C code, including some intended for future 
use. It has more than satisfied our initial requirements, although much more 
can be done. It aggregates common operations in a general but straightforward 
way. Its path cache is similar to Matuszekps file handle cache in his NFS cache,
and closer to 9P home, some of the subtle cases in handles management in 
fscfs seem to turn up in the implementation of 9P in the Linux kernel, 
where Linux lacks anything corresponding exactly to a 9P handle.

\section{New network communications models}
\subsection{Active Message Support}
\subsection{What are we calling Charles stuff?}
\section{Kittyhawk Kernels and VMM}

Supercomputers and clouds both strive to make a large number of 
computing cores available for computation. 
However, current cloud infrastructure does not yield the performance 
sought by many scientific applications. A source of the performance 
loss comes from virtualization and virtualization of the network in 
particular. 
Project Kittyhawk~\cite{kh-sciencecloud} was an undertaking at IBM Research 
to explore the use of a hybrid supercomputer software infrastructure
on top of a BlueGene/P which allows direct hardware access to the 
communication hardware for the necessary components while providing 
the standard elastic cloud infrastructure for other components.

Beyond the enabling infrastructure for cloud provisioning and dynamic
boot of multiple images across compute nodes within a single BG/P
allocation, the Kittyhawk team also ported a Linux compute node
implementation complete with an Ethernet interface to the high-performance
collective and torus networks~\cite{kh-systemsjournal}.  
This allowed Linux applications to run unmodified om Blue Gene compute 
nodes without incurring the performance 
overhead of reflecting sockets and I/O operations thorugh user space
helpers as in ZeptoOS.  It also enabled more conventional storage solutions
including SAN and NAS to be extended into the compute node network for
I/O intensive solutions.  While the Kittyhawk Linux kernel provided this
Ethernet interface, it also provided an API allowing the allocation of
torus channels directly to high performance applications which knew how to
interact with them allowing even greater degrees of performance and flexibility.
These allocated application channels co-existed with the kernel-space channels
providing a good hybrid trade off between system provided networking and
OS bypass.

The Kittyhawk environment was not limited to Linux kernel targets.  
It also supported execution of the L4 microkernel operating system.  
In addition to providing an alternative execution platform for applications,
the L4 Kittyhawk port also supported a virtual machine monitor which could
be used to further virtualize the BG/P nodes and network providing additional
protection in cloud deployment environments.

The Kittyhawk infrastructure, the linux kernel patches, and the L4 kernel
and virtual machine monitor have all been released as open source and are
available from http://kittyhawk.bu.edu.
