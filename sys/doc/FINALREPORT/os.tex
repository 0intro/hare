\chapter{Operating Systems Developments}
\section{Plan 9 Port to Blue Gene /L and /P}
\subsection{Large Pages}
Blue Gene   processors use software-loaded TLBs. A TLB manages the virtual to physical mapping for a single page, which 
in both Linux and Plan~9 is 4096 bytes. The processors support more than just one page size, in multiples of 4, from 1024 bytes to 1 Gbyte (with a few sizes missing). 
The CNK 
features 1 Mbyte TLBs, and benefits from reduced TLB fault interrupts. 

We extended Plan~9 to support larger TLBs. In order to measure the improvement we used the strid3 benchmark
\begin{figure}[h]
\begin{center}
 \includegraphics[width=4in]{strid3.pdf}
 \caption{{\bf Result of running strid3 on CNK, Plan 9, and Plan 9 with 1 MByte pages. }}
\label{strid3}
\end{center}
\end{figure}
to show the impact of TLB size upon an application. Shown in Figure \ref{strid3} is the result. Plan~9 is 
eight times slower than the CNK when 4096 byte pages are used. With one Mbyte pages in Plan~9, its 
performance is identical to the CNK. 
The change to Plan~9 to support the larger pages amounted to less than 30 lines of code\cite{DBLP:journals/ife/MinnichM09}. 

\section{Currying and Process-private system calls}
Extensive measurement of the kernel, using the tool developed and described in \cite{plan9trace}, showed
that for read and write system calls a great deal of overhead was repeated for each iteration of the 
call. Two of the biggest overheads are validation of the file descriptor and the read/write pointer and 
length validation. We also found in most cases that the pointers are being reused for each call: it is extremely rare for these parameters to be wrong, yet programs pay the full overhead for checking them 
each time. For the Blue Gene global barrier device, the overhead for the system call was 3.529 microseconds. 

In \cite{currying} we describe the possible approaches to resolving this problem. We developed two 
several extensions to Plan~9 that allowed the kernel to Curry the arguments to the read and write
system calls. The result: the file descriptor and pointer/length information is checked once, and the
result stored in a cache held in the proc struct (per-process information) in the kernel. The per-process structure is 
also extended with a private system call table. When the process makes a private system
call, all the standard checking is skipped and the pre-validated cached values are used instead. This 
new system call path resulted in a 5-fold reduction in overhead, to 729 nanoseconds. 

The Currying and per-process system calls together form a new type of interface to operating system
kernels. 

\subsection{Shared Heap}
Currying reduces the overhead for process-driven operations. It does nothing for interrupt-driven
operations, because interrupts, by design, do not occur in a process context. Page pinning is used 
to resolve this problem on Linux systems: process memory mappings are communicated to the driver
and ``pinned'' into place. Page pinning adds a great deal of code to a kernel; in some cases the
pinning code for (e.g.) Linux is larger than all of Plan~9 itself. 

We implemented a ``shared heap'' model in Plan~9. All process heaps, the kernel, and the drivers 
share a common address space, consisting of the top 1.5 Gbytes of memory. All heap (not text, stack,
or data, 
just heap) is allocated from this shared 1.5 Gbytes of memory. 
A given heap address in a process, kernel, or interrupt handler will
point to the same piece of memory. In contrast, in a standard OS, the same heap address in each 
process points to different memory. Note that a shared heap address space does not imply that 
all the data in the heap itself is shared between all the processes.  
Processes can share parts of the heap, but are typically limited to their own private piece, and 
can not see other processes heap data. 

Processes can easily pass pointers to each other because 
a given heap pointer has the same meaning in every process. Processes can communicate an address 
to the driver for use by the interrupt handler and there is no need for complex pinning, because the 
address has the same meaning in every machine context. 

We further modified the BG/P Torus driver to implement queues on the receive side. As packets are 
received  by the interrupt handler they are directly moved to the queues. 
\footnote{These queues are 
very similar to those provided on the T3D. In the T3D, however, they were implemented in hardware. }
Finally, once the driver and kernel were extended, we created a library to allow processes to 
send from the shared heap and receive into queues in the shared heap. 

Our resulting performance was better than that of the MPI library, even while using 
the kernel for I/O. In fact a simple ping-pong code ran with about 1/3 the latency 
of MPI. Our code was about 100 times smaller than the MPI library. Further, 
the device could be made to work for multiple independent applications, which is not possible
on BG/P hardware if the direct-access MPI libraries are used. We thus showed that if a 
kernel provides the proper abstractions, it is not necessary to run the network device driver
in user memory, as is done today. 

\section{File Systems}
\subsection{Compute Node Caches}
\section{New network communications models}
\subsection{Active Message Support}
\subsection{What are we calling Charles stuff?}
\section{Kittyhawk Kernels and VMM}

Supercomputers and clouds both strive to make a large number of 
computing cores available for computation. 
However, current cloud infrastructure does not yield the performance 
sought by many scientific applications. A source of the performance 
loss comes from virtualization and virtualization of the network in 
particular. 
Project Kittyhawk~\cite{kh-sciencecloud} was an undertaking at IBM Research 
to explore the use of a hybrid supercomputer software infrastructure
on top of a BlueGene/P which allows direct hardware access to the 
communication hardware for the necessary components while providing 
the standard elastic cloud infrastructure for other components.

Beyond the enabling infrastructure for cloud provisioning and dynamic
boot of multiple images across compute nodes within a single BG/P
allocation, the Kittyhawk team also ported a Linux compute node
implementation complete with an Ethernet interface to the high-performance
collective and torus networks~\cite{kh-systemsjournal}.  
This allowed Linux applications to run unmodified om Blue Gene compute 
nodes without incurring the performance 
overhead of reflecting sockets and I/O operations thorugh user space
helpers as in ZeptoOS.  It also enabled more conventional storage solutions
including SAN and NAS to be extended into the compute node network for
I/O intensive solutions.  While the Kittyhawk Linux kernel provided this
Ethernet interface, it also provided an API allowing the allocation of
torus channels directly to high performance applications which knew how to
interact with them allowing even greater degrees of performance and flexibility.
These allocated application channels co-existed with the kernel-space channels
providing a good hybrid trade off between system provided networking and
OS bypass.

The Kittyhawk environment was not limited to Linux kernel targets.  
It also supported execution of the L4 microkernel operating system.  
In addition to providing an alternative execution platform for applications,
the L4 Kittyhawk port also supported a virtual machine monitor which could
be used to further virtualize the BG/P nodes and network providing additional
protection in cloud deployment environments.

The Kittyhawk infrastructure, the linux kernel patches, and the L4 kernel
and virtual machine monitor have all been released as open source and are
available from http://kittyhawk.bu.edu.
